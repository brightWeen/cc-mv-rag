#!/usr/bin/env python3
"""
è¯„ä¼°è„šæœ¬

è¯„ä¼°å„æ£€ç´¢æ–¹æ³•çš„æ•ˆæœå¹¶ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
"""

import json
import sys
from pathlib import Path
from typing import Dict, List, Set

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from loguru import logger
from tabulate import tabulate

from src.config.settings import get_config
from src.evaluation.metrics import Evaluator


def load_queries(data_path: Path) -> list:
    """åŠ è½½æŸ¥è¯¢æ•°æ®"""
    with open(data_path, "r", encoding="utf-8") as f:
        return json.load(f)


def load_results(results_path: Path) -> dict:
    """åŠ è½½æ£€ç´¢ç»“æœ"""
    with open(results_path, "r", encoding="utf-8") as f:
        return json.load(f)


def prepare_relevant_docs(queries: list) -> Dict[str, Set[str]]:
    """å‡†å¤‡ç›¸å…³æ–‡æ¡£é›†åˆ"""
    relevant_docs = {}
    for query_item in queries:
        query_id = query_item["query_id"]
        relevant_docs[query_id] = set(query_item["relevant_docs"])
    return relevant_docs


def print_comparison_table(comparison: Dict[str, Dict[str, float]]):
    """æ‰“å°å¯¹æ¯”è¡¨æ ¼"""
    # è·å–æ‰€æœ‰æŒ‡æ ‡
    metrics = []
    for method_results in comparison.values():
        for key in method_results.keys():
            if key not in metrics and key != "query_count":
                metrics.append(key)
    metrics.sort()

    # æ„å»ºè¡¨æ ¼æ•°æ®
    headers = ["æ–¹æ³•"] + metrics
    rows = []

    for method_name, method_results in comparison.items():
        row = [method_name]
        for metric in metrics:
            value = method_results.get(metric, 0.0)
            if isinstance(value, float):
                row.append(f"{value:.4f}")
            else:
                row.append(str(value))
        rows.append(row)

    print("\n" + "=" * 80)
    print("æ£€ç´¢æ•ˆæœå¯¹æ¯”è¡¨")
    print("=" * 80)
    print(tabulate(rows, headers=headers, tablefmt="grid"))
    print("=" * 80 + "\n")


def print_ranking(comparison: Dict[str, Dict[str, float]]):
    """æ‰“å°å„æŒ‡æ ‡æ’å"""
    print("\n" + "=" * 80)
    print("å„æŒ‡æ ‡æ’åï¼ˆæŒ‰å€¼ä»é«˜åˆ°ä½ï¼‰")
    print("=" * 80 + "\n")

    # å…³é”®æŒ‡æ ‡
    key_metrics = ["recall@10", "mrr", "ndcg@10", "map@10"]

    for metric in key_metrics:
        print(f"ã€{metric.upper()}ã€‘")

        # æ’åº
        sorted_methods = sorted(
            comparison.items(),
            key=lambda x: x[1].get(metric, 0),
            reverse=True
        )

        for rank, (method_name, results) in enumerate(sorted_methods, 1):
            value = results.get(metric, 0)
            medal = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else "  "
            print(f"  {medal} {rank}. {method_name:20s} {value:.4f}")

        print()


def generate_markdown_report(
    comparison: Dict[str, Dict[str, float]],
    output_path: Path
):
    """ç”Ÿæˆ Markdown æŠ¥å‘Š"""
    report_lines = []

    report_lines.append("# Milvus å¤šè·¯æ£€ç´¢éªŒè¯æŠ¥å‘Š\n")
    report_lines.append("## è¯„ä¼°ç»“æœå¯¹æ¯”\n")

    # è¡¨æ ¼
    metrics = []
    for method_results in comparison.values():
        for key in method_results.keys():
            if key not in metrics and key != "query_count":
                metrics.append(key)
    metrics.sort()

    # Markdown è¡¨æ ¼
    report_lines.append("| æ–¹æ³• | " + " | ".join(metrics) + " |")
    report_lines.append("|" + "--|" * (len(metrics) + 1))

    for method_name, method_results in comparison.items():
        row_values = [f"{method_results.get(m, 0):.4f}" for m in metrics]
        report_lines.append(f"| {method_name} | " + " | ".join(row_values) + " |")

    report_lines.append("\n## ç»“è®º\n")

    # åˆ†æ
    report_lines.append("### å…³é”®æŒ‡æ ‡åˆ†æ\n")

    key_metrics = ["recall@10", "mrr", "ndcg@10", "map@10"]
    metric_descriptions = {
        "recall@10": "å¬å›ç‡@10 - å‰ 10 ä¸ªç»“æœä¸­ç›¸å…³æ–‡æ¡£çš„è¦†ç›–ç¨‹åº¦",
        "mrr": "å¹³å‡å€’æ•°æ’å - é¦–ä¸ªç›¸å…³æ–‡æ¡£çš„å¹³å‡æ’åè´¨é‡",
        "ndcg@10": "å½’ä¸€åŒ–æŠ˜æŸç´¯ç§¯å¢ç›Š@10 - è€ƒè™‘ä½ç½®çš„ç›¸å…³æ€§è´¨é‡",
        "map@10": "å¹³å‡ç²¾åº¦å‡å€¼@10 - æ•´ä½“æ£€ç´¢è´¨é‡"
    }

    for metric in key_metrics:
        sorted_methods = sorted(
            comparison.items(),
            key=lambda x: x[1].get(metric, 0),
            reverse=True
        )
        best_method = sorted_methods[0][0]
        best_value = sorted_methods[0][1].get(metric, 0)

        report_lines.append(f"#### {metric.upper()}")
        report_lines.append(f"- **æè¿°**: {metric_descriptions.get(metric, '')}")
        report_lines.append(f"- **æœ€ä½³æ–¹æ³•**: {best_method} ({best_value:.4f})")
        report_lines.append("")

    # å†™å…¥æ–‡ä»¶
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(report_lines))

    logger.info(f"Markdown æŠ¥å‘Šå·²ä¿å­˜: {output_path}")


def save_comparison_json(comparison: dict, output_path: Path):
    """ä¿å­˜å¯¹æ¯”ç»“æœä¸º JSON"""
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(comparison, f, ensure_ascii=False, indent=2)
    logger.info(f"å¯¹æ¯”ç»“æœå·²ä¿å­˜: {output_path}")


def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½é…ç½®
    config = get_config()

    # é…ç½®æ—¥å¿—
    logger.add(
        config.logging.file,
        rotation=config.logging.rotation,
        retention=config.logging.retention,
        level=config.logging.level
    )

    logger.info("å¼€å§‹è¯„ä¼°æ£€ç´¢ç»“æœ")
    logger.info(f"é¡¹ç›®: {config.project}")

    # é¡¹ç›®è·¯å¾„
    project_root = Path(__file__).parent.parent
    data_dir = project_root / "data"
    results_dir = project_root / "outputs" / "results"
    reports_dir = project_root / "outputs" / "reports"
    reports_dir.mkdir(parents=True, exist_ok=True)

    # åŠ è½½æŸ¥è¯¢æ•°æ®ï¼ˆåŒ…å« test_queries å’Œ mixed_queriesï¼‰
    queries_file = data_dir / "queries" / "test_queries.json"
    logger.info(f"åŠ è½½æŸ¥è¯¢æ•°æ®: {queries_file}")
    queries = load_queries(queries_file)
    logger.info(f"åŸºç¡€æŸ¥è¯¢æ•°é‡: {len(queries)}")

    # åŠ è½½æ··åˆæŸ¥è¯¢
    mixed_queries_file = data_dir / "queries" / "mixed_queries.json"
    if mixed_queries_file.exists():
        mixed_queries = load_queries(mixed_queries_file)
        queries.extend(mixed_queries)
        logger.info(f"æ··åˆæŸ¥è¯¢æ•°é‡: {len(mixed_queries)}")
        logger.info(f"æ€»æŸ¥è¯¢æ•°é‡: {len(queries)}")

    # å‡†å¤‡ç›¸å…³æ–‡æ¡£é›†åˆ
    relevant_docs = prepare_relevant_docs(queries)

    # åŠ è½½æ£€ç´¢ç»“æœ
    all_results_file = results_dir / "all_results.json"
    logger.info(f"åŠ è½½æ£€ç´¢ç»“æœ: {all_results_file}")
    all_results = load_results(all_results_file)

    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = Evaluator(k_values=config.evaluation.k_values)

    # è¯„ä¼°æ‰€æœ‰æ–¹æ³•
    logger.info("=" * 50)
    logger.info("å¼€å§‹è¯„ä¼°å„æ£€ç´¢æ–¹æ³•")
    logger.info("=" * 50)

    comparison = evaluator.compare_results(all_results, relevant_docs)

    # æ‰“å°ç»“æœ
    print_comparison_table(comparison)
    print_ranking(comparison)

    # ä¿å­˜ç»“æœ
    json_file = reports_dir / "comparison_results.json"
    save_comparison_json(comparison, json_file)

    md_file = reports_dir / "evaluation_report.md"
    generate_markdown_report(comparison, md_file)

    logger.info("=" * 50)
    logger.info("è¯„ä¼°å®Œæˆï¼")
    logger.info("=" * 50)
    logger.info(f"\nç»“æœæ–‡ä»¶:")
    logger.info(f"  - JSON: {json_file}")
    logger.info(f"  - Markdown: {md_file}")


if __name__ == "__main__":
    main()
